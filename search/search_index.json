{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Dataset","text":"<p>This is the documentation for IAHLT Arabic NER corpus and models.</p>"},{"location":"#corpus","title":"Corpus","text":""},{"location":"#schema-and-entity-types","title":"Schema and entity types","text":"<p>The corpus was annotated by IAHLT team. It contains 40,000 samples annotated with BILOU scheme as follows:   <code>B-</code> - the first token of a multi-token entity   <code>I-</code> - an inner token of a multi-token entity   <code>L-</code> - the last token of a multi-token entity   <code>U-</code> - a single-token entity (unit entity)    <code>O</code> - a non-entity token    </p> <p>And the following entity types:   <code>ANG</code> - Any named language (Hebrew, Arabic, English, French, etc.)     <code>DUC</code> - A branded product, objects, vehicles, medicines, foods, etc. (Apple, BMW, Coca-Cola, etc.)    <code>EVE</code> - Any named event (Olympics, World Cup, etc.)    <code>FAC</code> - Any named facility, building, airport, etc. (Eiffel Tower, Ben Gurion Airport, etc.)    <code>GPE</code> - Geo-political entity, nation states, counties, cities, etc.    <code>INFORMAL</code> - Informal language (slang)    <code>LOC</code> - Non-GPE locations, geographical regions, mountain ranges, bodies of water, etc.    <code>ORG</code> - Companies, agencies, institutions, political parties, etc.    <code>PER</code> - People, including fictional.    <code>TIMEX</code> - Time expression, absolute or relative dates or periods.    <code>TTL</code> - Any named title, position, profession, etc. (President, Prime Minister, etc.)    <code>WOA</code> - Any named work of art (books, movies, songs, etc.)    <code>MISC</code> - Miscellaneous entities, that do not belong to the previous categories     </p>"},{"location":"#statistics","title":"Statistics","text":"entity_tag count O 1419405 PER 68167 ORG 61642 GPE 56185 TIMEX 27759 MISC 22461 TTL 21765 LOC 13905 FAC 13658 WOA 9353 EVE 8427 DUC 5368 ANG 1964 INFORMAL 91"},{"location":"#links","title":"Links","text":"<p>The corpus is available in the following formats:</p> <ul> <li>Corpus - the original corpus </li> <li>Folds - the corpus split into 4 folds for cross-validation</li> </ul>"},{"location":"#fields","title":"Fields","text":"<p>Each sample contains the following fields:</p> <ul> <li><code>id</code> - the sample id</li> <li><code>tokens</code> - the list of tokens</li> <li><code>ner_tags</code> - the encoded list of NER tags in BILOU scheme</li> <li><code>raw_tags</code> - the list of NER tags in BILOU scheme</li> </ul>"},{"location":"#dataset-loading","title":"Dataset loading","text":"<p>Need to install the <code>datasets</code> library:</p> <p><pre><code>pip install datasets\n</code></pre> and login to the HuggingFace API using the given token:</p> <pre><code>huggingface-cli login # insert your token when prompted\n</code></pre> <p>Then you can load the corpus as follows:</p> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"iahlt/arabic_ner_mafat\", split=\"train\") # it has all the data in one split\nfor sample in dataset:\n    print(sample)\n</code></pre> <pre><code>from datasets import load_dataset\n\nfolds_dataset = {}\n\nfor fold in [\"fold_1\", \"fold_2\", \"fold_3\", \"fold_4\"]:\n    folds_dataset[fold] = load_dataset(\"iahlt/arabic_ner_mafat_folds\", fold)\n</code></pre>"},{"location":"evaluation/","title":"Evaluation","text":""},{"location":"evaluation/#evaluation-for-folds","title":"Evaluation for folds","text":"<p>For evaluation of the models on the folds, we use 4 folds cross-validation. So, as we have 40k samples, we split them into 4 folds of 10k samples on the validation. And then we trained and validated on each fold separately.</p>"},{"location":"evaluation/#fold-1","title":"Fold 1","text":"metrics eval_loss 0.00446131 eval_overall_precision 0.783601 eval_overall_recall 0.726919 eval_overall_f1 0.754196 eval_overall_accuracy 0.936082 eval_macro_avg.precision 0.665234 eval_macro_avg.recall 0.60265 eval_macro_avg.f1-score 0.630713 eval_macro_avg.support 35418 eval_weighted_avg.precision 0.780372 eval_weighted_avg.recall 0.726919 eval_weighted_avg.f1-score 0.751985 eval_weighted_avg.support 35418 eval_micro_avg.precision 0.783601 eval_micro_avg.recall 0.726919 eval_micro_avg.f1-score 0.754196 eval_micro_avg.support 35418 eval_runtime 1160.95 eval_samples_per_second 52.292 eval_steps_per_second 26.147"},{"location":"evaluation/#fold-2","title":"Fold 2","text":"metrics eval_loss 0.00431787 eval_overall_precision 0.782074 eval_overall_recall 0.712519 eval_overall_f1 0.745678 eval_overall_accuracy 0.933556 eval_macro_avg.precision 0.667987 eval_macro_avg.recall 0.58284 eval_macro_avg.f1-score 0.620279 eval_macro_avg.support 36959 eval_weighted_avg.precision 0.778731 eval_weighted_avg.recall 0.712519 eval_weighted_avg.f1-score 0.743216 eval_weighted_avg.support 36959 eval_micro_avg.precision 0.782074 eval_micro_avg.recall 0.712519 eval_micro_avg.f1-score 0.745678 eval_micro_avg.support 36959 eval_runtime 1255.45 eval_samples_per_second 52.914 eval_steps_per_second 26.457"},{"location":"evaluation/#fold-3","title":"Fold 3","text":"metrics eval_loss 0.00460493 eval_overall_precision 0.804563 eval_overall_recall 0.688634 eval_overall_f1 0.742098 eval_overall_accuracy 0.932954 eval_macro_avg.precision 0.694945 eval_macro_avg.recall 0.561292 eval_macro_avg.f1-score 0.617365 eval_macro_avg.support 36311 eval_weighted_avg.precision 0.798574 eval_weighted_avg.recall 0.688634 eval_weighted_avg.f1-score 0.737633 eval_weighted_avg.support 36311 eval_micro_avg.precision 0.804563 eval_micro_avg.recall 0.688634 eval_micro_avg.f1-score 0.742098 eval_micro_avg.support 36311 eval_runtime 1230.3 eval_samples_per_second 51.72 eval_steps_per_second 25.86"},{"location":"evaluation/#fold-4","title":"Fold 4","text":"metrics eval_loss 0.00429184 eval_overall_precision 0.779286 eval_overall_recall 0.728163 eval_overall_f1 0.752858 eval_overall_accuracy 0.936848 eval_macro_avg.precision 0.66828 eval_macro_avg.recall 0.604989 eval_macro_avg.f1-score 0.632891 eval_macro_avg.support 35639 eval_weighted_avg.precision 0.777074 eval_weighted_avg.recall 0.728163 eval_weighted_avg.f1-score 0.75088 eval_weighted_avg.support 35639 eval_micro_avg.precision 0.779286 eval_micro_avg.recall 0.728163 eval_micro_avg.f1-score 0.752858 eval_micro_avg.support 35639 eval_runtime 1220.92 eval_samples_per_second 52.255 eval_steps_per_second 26.128"},{"location":"evaluation/#average","title":"Average","text":"<p>Average of the folds with range(plus/minus 1.96 std, ~95% CI):</p> metrics eval_loss 0.00 \u00b1 0.00 eval_ANG.precision 0.81 \u00b1 0.04 eval_ANG.recall 0.77 \u00b1 0.04 eval_ANG.f1 0.79 \u00b1 0.04 eval_ANG.number 237, 243, 219, 243 eval_DUC.precision 0.69 \u00b1 0.02 eval_DUC.recall 0.57 \u00b1 0.04 eval_DUC.f1 0.62 \u00b1 0.02 eval_DUC.number 597, 555, 563, 551 eval_EVE.precision 0.61 \u00b1 0.05 eval_EVE.recall 0.58 \u00b1 0.03 eval_EVE.f1 0.59 \u00b1 0.01 eval_EVE.number 792, 937, 791, 833 eval_FAC.precision 0.62 \u00b1 0.04 eval_FAC.recall 0.47 \u00b1 0.03 eval_FAC.f1 0.53 \u00b1 0.01 eval_FAC.number 1399, 1393, 1302, 1321 eval_GPE.precision 0.86 \u00b1 0.01 eval_GPE.recall 0.82 \u00b1 0.01 eval_GPE.f1 0.84 \u00b1 0.01 eval_GPE.number 8524, 8840, 8537, 8483 eval_INFORMAL.precision 0.00 \u00b1 0.00 eval_INFORMAL.recall 0.00 \u00b1 0.00 eval_INFORMAL.f1 0.00 \u00b1 0.00 eval_INFORMAL.number 10, 15, 12, 14 eval_LOC.precision 0.66 \u00b1 0.02 eval_LOC.recall 0.56 \u00b1 0.03 eval_LOC.f1 0.61 \u00b1 0.02 eval_LOC.number 1631, 1644, 1626, 1644 eval_MISC.precision 0.70 \u00b1 0.02 eval_MISC.recall 0.59 \u00b1 0.01 eval_MISC.f1 0.64 \u00b1 0.01 eval_MISC.number 2398, 2535, 2575, 2616 eval_ORG.precision 0.74 \u00b1 0.02 eval_ORG.recall 0.67 \u00b1 0.02 eval_ORG.f1 0.71 \u00b1 0.01 eval_ORG.number 6265, 6411, 6415, 6326 eval_PER.precision 0.88 \u00b1 0.00 eval_PER.recall 0.83 \u00b1 0.02 eval_PER.f1 0.85 \u00b1 0.01 eval_PER.number 6946, 7451, 7394, 7148 eval_TIMEX.precision 0.82 \u00b1 0.01 eval_TIMEX.recall 0.75 \u00b1 0.02 eval_TIMEX.f1 0.78 \u00b1 0.01 eval_TIMEX.number 3082, 3216, 3126, 2969 eval_TTL.precision 0.69 \u00b1 0.04 eval_TTL.recall 0.63 \u00b1 0.05 eval_TTL.f1 0.66 \u00b1 0.01 eval_TTL.number 2815, 2980, 3025, 2768 eval_WOA.precision 0.69 \u00b1 0.03 eval_WOA.recall 0.41 \u00b1 0.03 eval_WOA.f1 0.52 \u00b1 0.02 eval_WOA.number 722, 739, 726, 723 eval_overall_precision 0.79 \u00b1 0.01 eval_overall_recall 0.71 \u00b1 0.02 eval_overall_f1 0.75 \u00b1 0.01 eval_overall_accuracy 0.93 \u00b1 0.00 eval_macro_avg.precision 0.67 \u00b1 0.01 eval_macro_avg.recall 0.59 \u00b1 0.02 eval_macro_avg.f1-score 0.63 \u00b1 0.01 eval_macro_avg.support 35418.0, 36959.0, 36311.0, 35639.0 eval_weighted_avg.precision 0.78 \u00b1 0.01 eval_weighted_avg.recall 0.71 \u00b1 0.02 eval_weighted_avg.f1-score 0.75 \u00b1 0.01 eval_weighted_avg.support 35418.0, 36959.0, 36311.0, 35639.0 eval_micro_avg.precision 0.79 \u00b1 0.01 eval_micro_avg.recall 0.71 \u00b1 0.02 eval_micro_avg.f1-score 0.75 \u00b1 0.01 eval_micro_avg.support 35418.0, 36959.0, 36311.0, 35639.0 eval_runtime 1216.91 \u00b1 40.05 eval_samples_per_second 52.292, 52.914, 51.72, 52.255 eval_steps_per_second 26.147, 26.457, 25.86, 26.128 fold fold_1, fold_2, fold_3, fold_4"},{"location":"training/","title":"Training","text":""},{"location":"training/#framework","title":"Framework","text":"<p>For the models training we used the following frameworks: <pre><code>pip install transformers\npip install span-marker\n</code></pre> where SpanMarker is a framework for training span-based models. It is built on top of the <code>Transformers</code> and uses the <code>Trainer</code> API from the <code>Transformers</code> library internally and implements the idea of  Packed Levitated Marker for Entity and Relation Extraction, D. Ye et al.,2022.</p> <p>So, generally speaking, it's <code>AutoModelForTokenClassification</code> from the <code>Transformers</code> library, but with additional technique.</p>"},{"location":"training/#training-script","title":"Training script","text":"<p>The training script is simple: <pre><code>import argparse\nimport json\nimport os\nfrom typing import Optional\n\nimport torch\nimport gc\nfrom span_marker import SpanMarkerModel\nfrom datasets import load_dataset, DatasetDict\nfrom transformers import TrainingArguments, AutoConfig\nfrom span_marker import Trainer\n\n\ndef load_ner_dataset(dataset_name: str, dataset_config_name: Optional[str] = None) -&gt; DatasetDict:\n    if dataset_config_name is None:\n        dataset = load_dataset(dataset_name)\n    else:\n        dataset = load_dataset(dataset_name, dataset_config_name)\n\n    def check_if_empty(l):\n        \"\"\"\n        Check if list is empty or contains only empty strings\n        :param l: \n        :return: \n        \"\"\"\n        if isinstance(l, list):\n            if len(l) == 0:\n                return True\n            elif len(l) == 1 and l[0] == \"\":\n                return True\n\n        if all([e == \"\" or e == \"\\u200b\" or e == '\\u200b\\u200b' for e in l]):\n            return True\n        if any([e is None for e in l]):\n            return True\n        return False\n\n    def remove_empty(record):\n        \"\"\"\n        Remove empty tokens and ner tags if any\n        \"\"\"\n        tokens = record[\"tokens\"]\n        ner_tags = record[\"ner_tags\"]\n\n        new_tokens = []\n        new_ner_tags = []\n        for token, ner_tag in zip(tokens, ner_tags):\n            # TODO: improve normalization for Arabic\n            token = token.replace(\"\\u200b\", \"\").replace(\"\\ufeff\", \"\").strip()\n            if token.strip():\n                new_tokens.append(token)\n                new_ner_tags.append(ner_tag)\n        record[\"tokens\"] = new_tokens\n        record[\"ner_tags\"] = new_ner_tags\n        return record\n\n    dataset[\"train\"] = dataset[\"train\"].filter(lambda x: not check_if_empty(x[\"tokens\"]))\n    dataset[\"validation\"] = dataset[\"validation\"].filter(lambda x: not check_if_empty(x[\"tokens\"]))\n    dataset[\"test\"] = dataset[\"test\"].filter(lambda x: not check_if_empty(x[\"tokens\"]))\n\n    dataset[\"train\"] = dataset[\"train\"].map(remove_empty)\n    dataset[\"validation\"] = dataset[\"validation\"].map(remove_empty)\n    dataset[\"test\"] = dataset[\"test\"].map(remove_empty)\n\n    return dataset\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model_name\", type=str, default=\"xlm-roberta-base\")\n    parser.add_argument(\"--dataset_name\", type=str)\n    parser.add_argument(\"--dataset_config_name\", type=str, default=None)\n    parser.add_argument(\"--entity_max_length\", type=str, default=150)\n    parser.add_argument(\"--lr\", type=float, default=1e-5)\n    parser.add_argument(\"--epochs\", type=int, default=3)\n    parser.add_argument(\"--warmup_ratio\", type=float, default=0.1)\n    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=2)\n    parser.add_argument(\"--per_device_train_batch_size\", type=int, default=8)\n    parser.add_argument(\"--per_device_eval_batch_size\", type=int, default=8)\n    parser.add_argument(\"--evaluation_strategy\", type=str, default=\"steps\")\n    parser.add_argument(\"--save_strategy\", type=str, default=\"steps\")\n    parser.add_argument(\"--save_steps\", type=int, default=1000)\n    parser.add_argument(\"--eval_steps\", type=int, default=1000)\n    parser.add_argument(\"--save_total_limit\", type=int, default=3)\n\n    args = parser.parse_args()\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    prefix = args.dataset_config_name if args.dataset_config_name else args.dataset_name\n    dataset = load_ner_dataset(args.dataset_name, args.dataset_config_name)\n    labels = dataset[\"train\"].features[\"ner_tags\"].feature.names\n    encoder_id = args.model_name\n    config = AutoConfig.from_pretrained(encoder_id)\n    model = SpanMarkerModel.from_pretrained(encoder_id,\n                                            labels=labels,\n                                            model_max_length=config.max_position_embeddings,\n                                            entity_max_length=args.entity_max_length)\n\n    os.makedirs(f\"models/{prefix}\", exist_ok=True)\n\n    args = TrainingArguments(\n        output_dir=f\"models/{prefix}\",\n        learning_rate=args.lr,\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        per_device_train_batch_size=args.per_device_train_batch_size,\n        per_device_eval_batch_size=args.per_device_eval_batch_size,\n        num_train_epochs=args.epochs,\n        evaluation_strategy=args.evaluation_strategy,\n        save_strategy=args.save_strategy,\n        save_steps=args.save_steps,\n        eval_steps=args.eval_steps,\n        push_to_hub=False,\n        logging_steps=50,\n        fp16=True,\n        warmup_ratio=args.warmup_ratio,\n        load_best_model_at_end=True,\n        save_total_limit=args.save_total_limit,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=dataset[\"train\"],\n        eval_dataset=dataset[\"validation\"],\n    )\n    trainer.train()\n    trainer.save_model(f\"models/{prefix}\")\n    metrics = trainer.evaluate()\n\n    with open(f\"models/{prefix}/metrics.json\", \"w\") as f:\n        f.write(json.dumps(metrics, indent=2))\n</code></pre></p>"},{"location":"training/#spanmarker-details","title":"SpanMarker details","text":""},{"location":"training/#explanation","title":"Explanation","text":"<p>Consider the following example input sentence: <code>\"Tom is my name.\"</code>, which tokenizes to this using the standard RoBERTa tokenizer: </p> \\[ \\begin{matrix} [0 &amp; 1560 &amp; 16 &amp; 127 &amp; 766 &amp; 4 &amp; 2] \\end{matrix} \\] <p>In addition to these tokens, we also have position IDs, which tells the RoBERTa encoder where in the text each of these tokens exist. In the above example, the position IDs are: $$ \\begin{matrix} [2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8] \\end{matrix} $$ For this example, we consider a maximum token length of 16 (note: this is unreasonably low, 256 or 512 would be more sensible in real scenarios). The SpanMarker codebase pads using 0's, so then the padded tokens (<code>input_ids</code>) are: $$ \\begin{matrix} [0 &amp; 1560 &amp; 16 &amp; 127 &amp; 766 &amp; 4 &amp; 2 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0] \\end{matrix} $$ And the position IDs now become: $$ \\begin{matrix} [2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 &amp; 11 &amp; 12 &amp; 13 &amp; 14 &amp; 15 &amp; 16 &amp; 17] \\end{matrix} $$</p>"},{"location":"training/#markers","title":"Markers","text":"<p>Crucially, the model takes advantage of \"markers\". These are special tokens that mark the start and the end of a span within the input sentence. Every single legal span within the input sentence corresponds with exactly one set of two markers. </p> <p>In this example, we recognize that the text consists of 5 words, including the dot. If we assume that an entity span is somewhere between 1 and 8 words, then this example has these 15 legal spans, indexed from 0 onwards: $$ \\begin{matrix} (0, 0) &amp; (0, 1) &amp; (0, 2) &amp; (0, 3) &amp; (0, 4) &amp; (1, 1) &amp; (1, 2) &amp; (1, 3)\\ (1, 4) &amp; (2, 2) &amp; (2, 3) &amp; (2, 4) &amp; (3, 3) &amp; (3, 4) &amp; (4, 4) \\end{matrix} $$ If one of the entities that we are interested in is \"Person\", then the \"(0, 0)\" span has label \"Person\", while all other spans have label \"NIL\" (or \"O\" for \"outside\"). In total, we have 15 spans each consisting of a start and an end index. </p> <p>SpanMarker uses the special tokens \"\\&lt;start&gt;\" with ID 50261 and \"\\&lt;end&gt;\" with ID 50262 to represent the starts and ends, respectively. These marker tokens are then appended to the padded text tokens (<code>input_ids</code>), tripling its size. Then, the position IDs are also updated to virtually position these markers between the texts. </p> <p>Note that the appended position IDs correspond exactly with the 15 legal spans. For example, the (1, 2) span is now represented using the tokens 50261 and 50262 with position IDs 3 and 4, as shown in bold in the following vectors.</p> <p>This results in the following input ID vector: $$ \\begin{matrix} [0 &amp; 1560 &amp; 16 &amp; 127 &amp; 766 &amp; 4 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 50261 &amp; 50261 &amp; 50261 &amp; 50261 &amp; 50261 &amp; 50261 &amp; \\textbf{50261} &amp; 50261\\ 50261 &amp; 50261 &amp; 50261 &amp; 50261 &amp; 50261 &amp; 50261 &amp; 50261 &amp; 0 \\ 50262 &amp; 50262 &amp; 50262 &amp; 50262 &amp; 50262 &amp; 50262 &amp; \\textbf{50262} &amp; 50262\\ 50262 &amp; 50262 &amp; 50262 &amp; 50262 &amp; 50262 &amp; 50262 &amp; 50262 &amp; 0] \\end{matrix} $$ And this position ID vector: $$ \\begin{matrix} [2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 \\ 10 &amp; 11 &amp; 12 &amp; 13 &amp; 14 &amp; 15 &amp; 16 &amp; 17 \\ 2 &amp; 2 &amp; 2 &amp; 2 &amp; 2 &amp; 3 &amp; \\textbf{3} &amp; 3 \\ 3 &amp; 4 &amp; 4 &amp; 4 &amp; 5 &amp; 5 &amp; 6 &amp; 0 \\ 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 3 &amp; \\textbf{4} &amp; 5 \\ 6 &amp; 4 &amp; 5 &amp; 6 &amp; 5 &amp; 6 &amp; 6 &amp; 0] \\end{matrix} $$ Whenever the number of legal spans is less than the maximum token length, like in our example, then the position IDs are padded with 0 such that the input ID and position ID vectors are still equal in length.</p> <p>Furthermore, if the number of legal spans is larger than the maximum token length, then we create multiple vectors, each using a section of all of the spans. This is used both in training and in inference.</p>"},{"location":"training/#attention","title":"Attention","text":"<p>The final piece of this complex puzzle is the attention mask matrix. This matrix gives us control over which tokens can attend to which other tokens. Generally, an attention mask vector is used, but using a matrix, we can specify a one-directional attention. This proves very useful for our situation.</p> <p></p> <p>See Figure 10 for the attention matrix used for our example. The block of black on the top left allows for self-attention of the text tokens, which is important for the RoBERTa encoder to correctly train the text. Note the dissymmetry: the sections of black on the left side allow for the start and end markers to attend to the text tokens, while the opposite is not possible. Furthermore, the four diagonal sections allow for the markers to attend to themselves and to the corresponding complementary marker.</p>"},{"location":"training/#encoder","title":"Encoder","text":"<p>With the input IDs, position IDs and attention mask matrix prepared, these can be fed through the pretrained RoBERTa encoder. This encoder returns an embedding for each input ID, resulting in an output shape of <code>[3 * max_tokens, embedding_size]</code>. The SpanMarker then computes a large embedding by concatenating the following vectors for each of the spans in the sample: 1. The embedding of the start marker. 2. The embedding of the end marker.</p> <p>This results in a feature vector of the shape <code>[max_tokens, 2 * embedding_size]</code>. This feature vector is fed through a linear layer to map it down from <code>2 * embedding_size</code> to <code>num_labels</code>. The resulting <code>[max_tokens, num_labels]</code> matrix is then used in a cross-entropy loss comparing it against labels for each of the spans in the sample.</p> <p>In our example, the feature vector for the (0, 0) span would then be compared against the gold label \"Person\", while all other feature vectors are compared against \"NIL\" or \"O\", or rather the integer representation of those labels.</p>"},{"location":"usage/","title":"Models","text":""},{"location":"usage/#setup","title":"Setup","text":""},{"location":"usage/#install-requirements","title":"Install requirements","text":"<pre><code>pip install datasets\npip install transformers\npip install span-marker\n</code></pre>"},{"location":"usage/#usage","title":"Usage","text":"<p>Models can be loaded from the HuggingFace Hub according to their name:</p> <ul> <li>iahlt/xlm-roberta-base-ar-ner-mafat-fold1</li> <li>iahlt/xlm-roberta-base-ar-ner-mafat-fold2</li> <li>iahlt/xlm-roberta-base-ar-ner-mafat-fold3</li> <li>iahlt/xlm-roberta-base-ar-ner-mafat-fold4</li> <li>iahlt/xlm-roberta-base-ar-ner-mafat (trained on 90%/10% train/validation split)</li> </ul> <p>But before you can load the model, you need to login to the HuggingFace API using the given token:</p> <pre><code>huggingface-cli login # insert your token when prompted\n</code></pre> <p>Then you can load the model and use it as follows:</p> <pre><code>from span_marker import SpanMarker\n\nmodel = SpanMarker.from_pretrained(\"iahlt/xlm-roberta-base-ar-ner-mafat\")\n</code></pre> <p>A model can be used to predict spans in a text and return a list of char-based spans:</p> <pre><code>text = \"\"\"\n\u064a\u0639\u064a\u0634 1.98 \u0645\u0644\u064a\u0648\u0646 \u0631\u062c\u0644 \u0648\u0627\u0645\u0631\u0623\u0629 \u0641\u064a \u0625\u0633\u0631\u0627\u0626\u064a\u0644 \u062a\u062d\u062a \u062e\u0637 \u0627\u0644\u0641\u0642\u0631\u060c \u0645\u0646 \u0628\u064a\u0646\u0647\u0645 873.3 \u0623\u0644\u0641 \u0637\u0641\u0644\u060c 152.5 \u0623\u0644\u0641 \u0645\u0648\u0627\u0637\u0646 \u0645\u0633\u0646 \u0648949.6 \u0623\u0644\u0641 \u0641\u064a \u0633\u0646 \u0627\u0644\u0639\u0645\u0644 \u2013 \u0647\u0630\u0627 \u0628\u062d\u0633\u0628 \u062a\u0642\u0631\u064a\u0631 \u0627\u0644\u0641\u0642\u0631 \u0644\u0639\u0627\u0645 2022 \u0627\u0644\u0635\u0627\u062f\u0631 \u0639\u0646 \u0645\u0624\u0633\u0633\u0629 \u0627\u0644\u062a\u0623\u0645\u064a\u0646 \u0627\u0644\u0648\u0637\u0646\u064a \u0648\u0627\u0644\u0630\u064a \u0635\u062f\u0631 \u064a\u0648\u0645 \u0627\u0644\u062e\u0645\u064a\u0633 (28/12) \u0648\u0628\u062d\u0633\u0628 \u0627\u0644\u062a\u0642\u0631\u064a\u0631\u060c \u064a\u0628\u0644\u063a \u0645\u0639\u062f\u0644 \u0627\u0644\u0641\u0642\u0631 \u0628\u064a\u0646 \u0639\u0627\u0645\u0629 \u0627\u0644\u0633\u0643\u0627\u0646 20.2%\u060c \u062f\u0648\u0646 \u062a\u063a\u064a\u064a\u0631 \u0639\u0646 \u0639\u0627\u0645 2021.\n\"\"\".strip()  # https://ar.davar1.co.il/479583/\nspans = model.predict(text)\nprint(spans)\n# [{'span': ['\u0625\u0633\u0631\u0627\u0626\u064a\u0644'],\n#   'label': 'GPE',\n#   'score': 0.9982821941375732,\n#   'word_start_index': 6,\n#   'word_end_index': 7},\n#  {'span': ['\u0644\u0639\u0627\u0645', '2022'],\n#   'label': 'TIMEX',\n#   'score': 0.9435986280441284,\n#   'word_start_index': 29,\n#   'word_end_index': 31},\n#  {'span': ['\u0645\u0624\u0633\u0633\u0629', '\u0627\u0644\u062a\u0623\u0645\u064a\u0646', '\u0627\u0644\u0648\u0637\u0646\u064a'],\n#   'label': 'ORG',\n#   'score': 0.967919647693634,\n#   'word_start_index': 33,\n#   'word_end_index': 36},\n#  {'span': ['\u064a\u0648\u0645', '\u0627\u0644\u062e\u0645\u064a\u0633'],\n#   'label': 'TIMEX',\n#   'score': 0.9344639182090759,\n#   'word_start_index': 38,\n#   'word_end_index': 40},\n#  {'span': ['\u0639\u0627\u0645', '2021.'],\n#   'label': 'TIMEX',\n#   'score': 0.9829164147377014,\n#   'word_start_index': 53,\n#   'word_end_index': 55}]\n</code></pre> <p>A model can be used to predict spans over a pre-tokenized text (returns a list of token-based spans):</p> <pre><code>segmented_text = text.split()  # your tokenization/segmentation of text\nprint(model.predict(segmented_text))\n# [{'span': ['\u0625\u0633\u0631\u0627\u0626\u064a\u0644'], 'label': 'GPE', 'score': 0.9982821941375732, 'word_start_index': 6, 'word_end_index': 7}, {'span': ['\u0644\u0639\u0627\u0645', '2022'], 'label': 'TIMEX', 'score': 0.9435986280441284, 'word_start_index': 29, 'word_end_index': 31}, {'span': ['\u0645\u0624\u0633\u0633\u0629', '\u0627\u0644\u062a\u0623\u0645\u064a\u0646', '\u0627\u0644\u0648\u0637\u0646\u064a'], 'label': 'ORG', 'score': 0.967919647693634, 'word_start_index': 33, 'word_end_index': 36}, {'span': ['\u064a\u0648\u0645', '\u0627\u0644\u062e\u0645\u064a\u0633'], 'label': 'TIMEX', 'score': 0.9344639182090759, 'word_start_index': 38, 'word_end_index': 40}, {'span': ['\u0639\u0627\u0645', '2021.'], 'label': 'TIMEX', 'score': 0.9829164147377014, 'word_start_index': 53, 'word_end_index': 55}]\n</code></pre>"},{"location":"usage/#usage-with-spacy-optional","title":"Usage with spacy (optional)","text":"<p>It's possible to use the model with spacy, to do so, you need to install spacy and spacy-transformers and stanza for Arabic (multiword tokenization) MWT(segmentation)</p>"},{"location":"usage/#install-requirements_1","title":"Install requirements","text":"<pre><code>pip install spacy\npip install spacy-transformers\npip install stanza spacy_stanza\n</code></pre> <pre><code>import spacy\nimport stanza\nimport spacy_stanza\nfrom huggingface_hub import hf_hub_download\n\nstanza.download(\"ar\")\nnlp = spacy_stanza.load_pipeline(\"ar\", processors=\"tokenize,pos,lemma,depparse\")\nnlp.add_pipe(\"span_marker\", config={\"model\": \"iahlt/xlm-roberta-base-ar-ner-mafat\"})\n</code></pre> <p>And then you can use the model with spacy as usual:</p> <pre><code>doc = nlp(text)\n\nfor ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n# \u0625\u0633\u0631\u0627\u0626\u064a\u0644 31 38 GPE\n# \u0639\u0627\u0645 2022 150 158 TIMEX\n# \u0645\u0624\u0633\u0633\u0629 \u0627\u0644\u062a\u0623\u0645\u064a\u0646 \u0627\u0644\u0648\u0637\u0646\u064a 169 189 ORG\n# \u064a\u0648\u0645 \u0627\u0644\u062e\u0645\u064a\u0633 201 211 TIMEX\n# 28 / 12 ) 214 223 TIMEX\n# \u0639\u0627\u0645 2021 . 296 306 TIMEX\n</code></pre> <p>visualize the entities:</p> <pre><code>from spacy import displacy\n\ndisplacy.render(doc, style=\"ent\", jupyter=True)\n</code></pre>"}]}